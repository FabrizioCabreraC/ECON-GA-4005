{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning &#x2013; TD learning\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Linear Algebra\n",
    "- Statistics and Probability\n",
    "- Dynamic Programming\n",
    "- Reinforcement Learning Introduction\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the meaning of the $Q(s, a)$ function\n",
    "- Understand the concept of a temporal difference\n",
    "- Apply temporal differences to form an RL algorithm (Sarsa)\n",
    "\n",
    "**References**\n",
    "\n",
    "- Barto & Sutton book (online by authors [here](http://incompleteideas.net/book/the-book.html)) chapters 4-6\n",
    "- [Stokey and Lucas (1989)](https://www.jstor.org/stable/j.ctvjnrt76) Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Reminder: Dynamic programming\n",
    "\n",
    "- Let's begin by recalling what we know about dynamic programming\n",
    "- Recall the cake eating problem:\n",
    "    - Time is discrete\n",
    "    - $\\beta$ is discount factor\n",
    "    - Size of cake is $\\bar{x}$\n",
    "    - Consumption of cake in period $t$ is $c_t$\n",
    "    - Utility function $u: \\mathbb{R} \\rightarrow \\mathbb{R}$ maps from consumption today into happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sequential Problem\n",
    "\n",
    "- Objective\n",
    "$$\\begin{aligned}\n",
    "\\max_{c_t} &\\sum_{t=0}^{\\infty} \\beta^t u(c_t) \\\\\n",
    "\\text{subject to } \\quad & \\sum_{t=0}^{\\infty} c_t \\le \\bar{x} \\\\\n",
    "& c_t \\ge 0 \\quad \\forall t\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Need to solve for *infinite* sequence $\\{c_t\\}_t$\n",
    "- Or..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Value Function\n",
    "\n",
    "- We can set up a value function $$v(\\bar{x}) \\equiv \\sum_{t=0}^{\\infty} \\beta^t u(c_t)$$\n",
    "- $v(\\bar{x})$ is the total *value* the consumer places on having a cake of size $\\bar{x}$\n",
    "- Decompose $v$ into two steps: first period + later periods $$v(\\bar{x}) = \\underbrace{u(c_0)}_{\\text{flow utility}} + \\underbrace{\\beta \\sum_{t=1}^{\\infty} \\beta^{t-1} u(c_t)}_{\\text{continuation utility}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Recursive Formulation\n",
    "\n",
    "- Note: continuation utility depends on $x_{t+1} = x_t - c_t$\n",
    "- Use this observation to write $v: \\mathbb{R}^+ \\rightarrow \\mathbb{R}$ recursively:\n",
    "$$\\begin{aligned}\n",
    "  v(x_t) &= \\max_{0 \\leq c_t  \\leq x} \\underbrace{u(c_t)}_{\\text{flow utility}} + \\underbrace{\\beta v(x_t - c_t)}_{\\text{continuation value}}\n",
    "\\end{aligned}$$\n",
    "- This is known as the **Bellman Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Solution to Recursive Problem\n",
    "\n",
    "- A solution to the dynamic program consists of two functions:\n",
    "    1. **Value function** $v^*(x): \\mathbb{R}^+ \\rightarrow \\mathbb{R}$ -- value of beginning period with $x$ cake remaining\n",
    "    2. **Policy function** $c^*(x): \\mathbb{R}^+ \\rightarrow [0, x]$ -- optimal level of consumption with $x$ cake remaining\n",
    "- Under certain regularity conditions (which we assume), the recursive problem (and its solution) is equivalent to the sequential problem we started with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Connection to RL\n",
    "\n",
    "- How does this connect to RL?\n",
    "- (S, A, R) pattern for RL is very closely related to recursive formulation of dynamic programming\n",
    "    - $S_t \\Longrightarrow x_t$\n",
    "    - $A_t \\Longrightarrow c_t$ \n",
    "    - $R_t \\Longrightarrow u(c_t)$\n",
    "- Expressing $v(x) = \\text{flow utility} + \\text{ continuation value}$ is like repeating (S, A, R) sequence many times\n",
    "- Baseline algorithm for solving DP problem (VFI) is quite similar to how basic RL algorithms work\n",
    "    - Start with guess for value, make decision, update guess, repeat..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## TD-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Baseline Assumptions\n",
    "\n",
    "- Let state space $\\mathcal{S}$ and action space $\\mathcal{A}$ be discrete\n",
    "- Let $S_t \\in \\mathcal{S}$  represent state at time $t$\n",
    "- Let $A_t \\in \\mathcal{A}(S_t)$ represent action at time $t$\n",
    "- Let $R_{t+1} \\in \\mathcal{R} \\subseteq \\mathbb{R}$ represent reward at time $t+1$\n",
    "- Let state transitions satisfy the Markov property such that \n",
    "$$\n",
    "\\begin{aligned}\n",
    "& p(s', r | s, a) = \\text{Prob}(S_{t+1}=s', R_{t+1}=r | S_{t} = s, A_{t} = 1) \\\\\n",
    "\\text{ where } \\quad & \\sum_{s' \\in \\mathcal{S}} \\sum_{R \\in \\mathcal{R}} p(s', r | s, a) = 1 \\quad \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(S) \\\\\n",
    "\\text{ and } \\quad & p(s', r | s, a) \\ge = 0 \\quad \\forall s, s' \\in \\mathcal{S}, a \\in \\mathcal{A}(s), r \\in \\mathcal{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> Note *Markov* means that probability for $S_{t+1}, R_{t+1}$ only depends on $S_t, A_t$ and not and $S_i, A_i, R_i$ where $i < t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### State Value Function\n",
    "\n",
    "- Let $v^*(s)$ be the optimal value of being in state $s$ (called *state value function*)\n",
    "- We write this as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^*(s) &= \\max_{a \\in \\mathcal{A}(s)} E \\left[R_{t+1} + \\beta v^*(S_{t+1}) | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\max_{a \\in \\mathcal{A}(s)} \\sum_{s', r} p(s', r | s, a) \\left[ r + \\beta v^*(s') \\right]\n",
    "\\end{aligned}$$\n",
    "- Should be familiar from our dynamic programming studies\n",
    "- Note expectation around the flow utility term $R_{t+1}$, leaving room for that reward to be stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Action Value Function\n",
    "\n",
    "- We can also write an *action value function*\n",
    "- Let $q^*(s, a)$ be the optimal value of being in state $s$ and choosing action $a$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q^*(s, a) &= E \\left[R_{t+1} + \\beta \\max_{a' \\in \\mathcal{A}(S_{t+1})} q^*(S_{t+1}, a') | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) \\left[ r + \\beta \\max_{a' \\in \\mathcal{A}(S_{t+1})} q^*(s', a') \\right]\n",
    "\\end{aligned}$$\n",
    "- Notice max operator is now *inside* the expectation and applied to future decision $a'$\n",
    "- The function $q^*(s, a)$ is more general than $v^*(s)$: $$v^*(s) = \\max_{a \\in \\mathcal{A}(s)} q^*(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Acting with $q^*$\n",
    "\n",
    "- Goal of RL is to learn to make decisions that maximize $\\sum \\beta^t R_t$\n",
    "- Knowing $q^*(s, a)$ tells us maximium value of being in state $s$ and choosing $a$\n",
    "- If we *knew* $q^*$, acting optimally would be easy: $$a^*(s) = \\text{argmax}_{a\n",
    "           \\in \\mathcal{A}(s)} q^*(s, a)$$\n",
    "- However, we rarely if ever *know* $q^*$, so we must approximate it\n",
    "- We will let $Q(s, a)$ represent our *approximation* of $q^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Approximating $q^*$\n",
    "\n",
    "- The goal of TD learning is to find an accurate approximation $Q(s, a)$ such that $Q(s, a) \\approx q^*(s, a) \\; \\forall s, a)$\n",
    "- There are many RL algorithms that seek to do this\n",
    "- We'll focus on two:\n",
    "    - Sarsa\n",
    "    - Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Temporal Differences\n",
    "\n",
    "\n",
    "- The Bellman equaition for our approximation $Q(s, a)$ is $$Q(s,a) = E[R' + \\beta \\max_{a'} Q(s', a') | s,a]$$\n",
    "- Suppose that we interacted with environment and have in hand $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "- Now we plug these into the Bellman by:\n",
    "    1. Using the form of the Bellman\n",
    "    2. But drop $E$ and $\\max$ because we already know the transition that did occur from $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "    3. Change $=$ an $\\approx$ because this isn't evaluating full Bellman\n",
    "$$Q(S_t,A_t) \\approx R_{t+1} + \\beta  Q(S_{t+1}, A_{t+1})$$\n",
    "- The difference between the left and right and sides is known as a temporal difference: $$TD(0)(Q) \\equiv R_{t+1} + \\beta Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$$\n",
    "\n",
    "> There are extensions to the temporal difference that allow for multiple time periods. The 0 in $TD(0)$ indicates that this is *one-step* TD learning. See Chapters 7 and 12 of Sutton/Barto for more info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Learning using TD(0)\n",
    "\n",
    "- We can use temporal differences to *improve* our approximation $Q$:\n",
    "- Let $Q_t(s, a)$ represent our approximation at the start of period $t$\n",
    "- Similar to gradient descent methods, we will take a step from $Q_t$ in a direction that improves its accuracy\n",
    "- To improve accuracy we step in direction of $TD(0)(Q_t)$ (using step size $\\alpha$): \n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{t+1}(S_t, A_t) &= Q_t(S_t, A_t) + \\alpha TD(0)(Q_t) \\\\\n",
    "&= Q_t(S_t, A_t) + \\alpha \\left[R_{t+1} + \\beta Q_t(S_{t+1}, A_{t+1}) - Q_t(S_t, A_t) \\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sarsa Algorithm\n",
    "\n",
    "- The Sarsa algorithm applies the update rule we just described\n",
    "- The algorithm is summarized by Barto and Sutton as follows (section 6.4)\n",
    "\n",
    "![sarsa_barto_sutton.png](./sarsa_barto_sutton.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQ:\n",
    "    def __init__(self):\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "    \n",
    "    def __call__(self, s, a):\n",
    "        return self.Q[(s.observable_state(), a)]\n",
    "\n",
    "    def __setitem__(self, k, v):\n",
    "        s, a = k\n",
    "        self.Q[(s.observable_state(), a)] = v\n",
    "        \n",
    "    def get_greedy(self, s, A_s):\n",
    "        vals = [self(s, a) for a in A_s]\n",
    "        max_val = max(vals)\n",
    "        return random.choice([a for (a, v) in zip(A_s, vals) if v == max_val])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sarsa(object):\n",
    "    def __init__(self, environment, epsilon=0.9, alpha=0.1, beta=1.0):\n",
    "        self.env = environment\n",
    "        self.Q = TabularQ()\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.restart_episode()    \n",
    "\n",
    "    def restart_episode(self):\n",
    "        # These will be (S, A) in our notation. Need to initialize\n",
    "        self.s = self.env.reset()\n",
    "        self.a = self.act(self.s, self.env.enumerate_options(self.s))        \n",
    "\n",
    "    def get_greedy(self, s, A_s):\n",
    "        return self.Q.get_greedy(s, A_s)\n",
    "    \n",
    "    def act(self, s, A_s):\n",
    "        if random.random() > self.epsilon:\n",
    "            return random.choice(A_s)\n",
    "        return self.get_greedy(s, A_s)\n",
    "\n",
    "    def done(self, s=None) -> bool:\n",
    "        return self.env.done(s if s else self.s)\n",
    "    \n",
    "    def step(self):\n",
    "        # first take the step (s, a)\n",
    "        s, a = self.s, self.a\n",
    "        sp, r = self.env.step(s, a)\n",
    "        \n",
    "        if self.done(sp):\n",
    "            # game is over\n",
    "            self.s = sp\n",
    "            return\n",
    "        \n",
    "        # then use policy to compute ap\n",
    "        A_sp = self.env.enumerate_options(sp)\n",
    "        ap = self.act(sp, A_sp)\n",
    "        \n",
    "        # now we know S-A-R-S'-A' -- ready to do update\n",
    "        Q, α, β = self.Q, self.alpha, self.beta  # simplify notation\n",
    "        Q[(s, a)] = Q(s, a) + α * (r + β * Q(sp, ap) - Q(s, a))\n",
    "        \n",
    "        # step forward in time\n",
    "        self.s = sp \n",
    "        self.a = ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example: Farkle\n",
    "\n",
    "- In a separate video we implemented the dice game farkle\n",
    "- We'll re-use that code as an environment for `Sarsa` algorithm\n",
    "- For a review of farkle, see video\n",
    "- Today we'll approach it like the RL algorihtm will: \n",
    "    - A stochastic environment that sends states, a list of possible actions, and rewards\n",
    "    - We will *not* specialize based on rules of game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Farkle env\n",
    "\n",
    "- Let's wrap farkle code into environment `Sarsa` expects\n",
    "- Need a few key methods:\n",
    "    - `reset() -> State`\n",
    "    - `enumerate_options(state) -> List[Action]`\n",
    "    - `step(s: State, a: Action) -> Tuple[State, Reward]`\n",
    "    - `done(s:State) -> bool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farkle import State, Action, RandomFarklePlayer, FarklePlayer, STOP, BANKRUPT\n",
    "from typing import List, Tuple\n",
    "\n",
    "class FarkleEnv:    \n",
    "    # first, some helper methods\n",
    "    def __init__(\n",
    "            self, \n",
    "            opponent: FarklePlayer=RandomFarklePlayer(), \n",
    "            points_to_win=10_000, \n",
    "            verbose: bool = False\n",
    "        ):\n",
    "        self.points_to_win = points_to_win\n",
    "        self.opponent = opponent\n",
    "        self.n_players = 2\n",
    "        self._state = State(self.n_players)\n",
    "        self._history: List[Tuple[State, Action]] = []\n",
    "\n",
    "    @property\n",
    "    def state(self) -> State:\n",
    "        return self._state\n",
    "\n",
    "    def set_state(self, action: Action, new_state: State):\n",
    "        self._history.append((self.state, action))\n",
    "        self._state = new_state\n",
    "    \n",
    "    def opponent_turn(self, s: State) -> State:\n",
    "        choices = s.enumerate_options()         \n",
    "        action = self.opponent.act(s, choices)\n",
    "        sp = s.step(action)\n",
    "\n",
    "        # check if player chose to stop\n",
    "        if sp.current_player != 1:\n",
    "            return sp\n",
    "\n",
    "        # Player didn't stop, but still their turn. Call again\n",
    "        return self.opponent_turn(sp)\n",
    "        \n",
    "    # key methods needed\n",
    "    def done(self, state) -> bool:\n",
    "        return any(score > self.points_to_win for score in state.scores)\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = State(self.n_players)\n",
    "        self._history = []\n",
    "        return self.state.roll()\n",
    "\n",
    "    def step(self, s: State, a: Action) -> Tuple[State, int]:\n",
    "        sp = s.step(a)\n",
    "        r = 0\n",
    "        \n",
    "        # see if we ended\n",
    "        if sp.current_player != 0:\n",
    "            if a is STOP:  \n",
    "                # only score when we choose to stop\n",
    "                r = s.turn_sum\n",
    "            \n",
    "            # take opponent turn\n",
    "            sp = self.opponent_turn(sp)\n",
    "            \n",
    "        self.set_state(a, sp)\n",
    "        return sp, r\n",
    "    \n",
    "    def enumerate_options(self, s: State) -> List[Action]:\n",
    "        return self.state.enumerate_options(s.rolled_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Playing Farkle\n",
    "\n",
    "- Let's try it out!\n",
    "- We need to create an env, then pass it to sarsa\n",
    "- We'll also define a helper function to play a game for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FarkleEnv()\n",
    "sarsa = Sarsa(env, epsilon=0.9, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def play_game(algo):\n",
    "    algo.restart_episode()\n",
    "    while not algo.done():\n",
    "        algo.step()\n",
    "    return algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Round: 144. Score: [10200, 8150]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game(sarsa)\n",
    "sarsa.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd64aa47e80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = [h[0].scores for h in env._history]\n",
    "plt.plot(scores)\n",
    "plt.legend([\"sarsa\", \"random\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farkle import HumanFarklePlayer\n",
    "env_human = FarkleEnv(opponent=HumanFarklePlayer(name=\"Spencer\"), points_to_win=2000)\n",
    "sarsa_human = Sarsa(env_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(score: [250, 0]) You have rolled:  [] ( [] )\n",
      "The current total you have at stake is: 0\n",
      "Your scoring options are as follows:\n",
      "\t0: roll\n",
      "\t1: stop\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4592152b79ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msarsa_human\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-da868941dba7>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(algo)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestart_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e11d765ec43d>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# first take the step (s, a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0c7386ac2e5b>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, s, a)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# take opponent turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopponent_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0c7386ac2e5b>\u001b[0m in \u001b[0;36mopponent_turn\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopponent_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mchoices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ECON-GA-4005.1/Handouts/L19_rl_2_tdlearning/farkle.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, choices)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Choose an integer to select a scoring option: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                 \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "play_game(sarsa_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Round: 0. Score: [0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa_human.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Learning\n",
    "\n",
    "- Great! Our algorithm can play Farkle\n",
    "- But... it needs to play *many* games to learn how to play well\n",
    "- Let's let it play many more games to build up some intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0/200000 (len(Q) = 505)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def play_many_games(N):\n",
    "    terminal_states = []\n",
    "    print_skip = N // 10\n",
    "    for i in range(N):\n",
    "        play_game(sarsa) \n",
    "        terminal_states.append(sarsa.s)\n",
    "        if i % print_skip == 0:\n",
    "            print(f\"Done with {i}/{N} (len(Q) = {len(sarsa.Q.Q)})\")\n",
    "    return terminal_states\n",
    "\n",
    "# WARNING: this takes a *long time* and requires a lot of ram!\n",
    "# Only use on a computer with at least 32 GB ram\n",
    "# There are ways we could optimize this... such as only including\n",
    "# final score in `terminal_states` and dropping things like current_round\n",
    "# from the state\n",
    "sarsa_history = play_many_games(200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Did we learn?\n",
    "\n",
    "- Let's analyze the history and see if the algorithm learned with experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "won = np.array([s.scores[0] > s.scores[1] for s in sarsa_history])\n",
    "game_idx = np.arange(len(won))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(game_idx, won.cumsum())\n",
    "ax.plot(game_idx, 0.5 * game_idx)\n",
    "plt.legend([\"sarsa\", \"E[random agent]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "31582b9feba862c420bc95ad7fac43fb721c474490d1710b4e50ac63470f9531"
  },
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
