{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f607f6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "* Tensorflow + keras\n",
    "* CNN 1D\n",
    "\n",
    "Outcomes:\n",
    "\n",
    "* Understand the ideas behind recurrent neural networks\n",
    "* Apply recurrent neural networks to inflation prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084adcbe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e73ae2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "Many types of data are inherently sequential. The most common type of sequential data that you'll encounter is time-series data, but text data could also be viewed as \"sequential data\" since the words have an ordering to them.\n",
    "\n",
    "Today's lecture will focus primarily on time-series data, but these methods could be applied to other kinds of sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f84f35a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Today's data**\n",
    "\n",
    "We are loosely going to follow some of the work done in [\"Predicting Inflation with Neural Networks\"](https://warwick.ac.uk/fac/soc/economics/research/workingpapers/2021/twerp_1344_-_paranhos.pdf) by [Livia Paranhos](https://sites.google.com/view/livia-paranhos/research). Livia is a current PhD student at University of Warwick -- The tools we're discussing today are being actively applied by PhD students to do research!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30997c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In her paper, she uses 100+ macroeconomic variables from the FRED database as collected by McCracken Ng (2016) to predict inflation in a variety of settings.\n",
    "\n",
    "We are going to use a subset of the data that she uses to simplify our exposition, but we recommend skimming her paper or her slides to see how she used the larger dataset.\n",
    "\n",
    "In particular, we use:\n",
    "\n",
    "* `RPI`: Real personal income\n",
    "* `UNRATE`: Civilian unemployment rate\n",
    "* `PAYEMS`: Total civilian nonfarm employment\n",
    "* `TB3MS`: The three month treasury bill rate\n",
    "* `CPIAUCSL`: The consumer price index for all goods\n",
    "* `SP500`: The S&P 500 index price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57433ee0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"RPI\", \"UNRATE\", \"PAYEMS\",\n",
    "    \"TB3MS\", \"CPIAUCSL\", \"S&P 500\"\n",
    "]\n",
    "\n",
    "data_url = \"https://files.stlouisfed.org/\"\n",
    "data_url += \"files/htdocs/fred-md/monthly/2019-01.csv\"\n",
    "data = pd.read_csv(data_url).iloc[1:, :]\n",
    "\n",
    "data[\"dt\"] = pd.to_datetime(data[\"sasdate\"])\n",
    "\n",
    "data = data.set_index(\"dt\").loc[:, variables].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229cc03",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fc698",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once we've downloaded the data, we're going to perform some typical transformations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd866e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "diff = lambda x: x.diff(12)\n",
    "log_diff = lambda x: 100*np.log(x).diff(12)\n",
    "\n",
    "df = data.transform(\n",
    "    {\n",
    "        \"RPI\": log_diff,\n",
    "        \"UNRATE\": diff,\n",
    "        \"PAYEMS\": log_diff,\n",
    "        \"TB3MS\": diff,\n",
    "        \"CPIAUCSL\": log_diff,\n",
    "        \"S&P 500\": log_diff\n",
    "    }\n",
    ").dropna().loc[\"1970\":, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c96af1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20614648",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Training, validation, and test data**\n",
    "\n",
    "We'll use a 70%-20%-10% split for the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f4072",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n = df.shape[0]\n",
    "\n",
    "# Training data\n",
    "train_n = int(n*0.7)\n",
    "train_df = df.iloc[0:train_n, :]\n",
    "\n",
    "# Validation data\n",
    "val_n = int(n*0.2)\n",
    "val_df = df.iloc[train_n:train_n+val_n, :]\n",
    "\n",
    "# Testing data\n",
    "test_df = df.iloc[train_n+val_n:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c2dbe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Normalize the data**\n",
    "\n",
    "It is typical to modify our data to have mean 0 and standard deviation 1.\n",
    "\n",
    "We do this to the training data and then use the mean and standard deviation of the _training data_ to modify the validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea150a3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379cd50",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.plot(y=\"CPIAUCSL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400fb7b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "val_df.plot(y=\"CPIAUCSL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0997547",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_df.plot(y=\"CPIAUCSL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265e83c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Helper classes and functions**\n",
    "\n",
    "Follow the ideas in https://www.tensorflow.org/tutorials/structured_data/time_series and create a few helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e5d00",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "\n",
    "    def __init__(\n",
    "        self, input_width, label_width, shift,\n",
    "        train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "        label_columns=None\n",
    "    ):\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {\n",
    "                name: i for i, name in enumerate(label_columns)\n",
    "            }\n",
    "        self.column_indices = {\n",
    "            name: i for i, name in enumerate(train_df.columns)\n",
    "        }\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join(\n",
    "            [\n",
    "                f'Total window size: {self.total_window_size}',\n",
    "                f'Input indices: {self.input_indices}',\n",
    "                f'Label indices: {self.label_indices}',\n",
    "                f'Label column name(s): {self.label_columns}'\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [\n",
    "                    labels[:, :, self.column_indices[name]]\n",
    "                    for name in self.label_columns\n",
    "                ], axis=-1\n",
    "            )\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def plot(self, model=None, plot_col='CPIAUCSL', max_subplots=3):\n",
    "    \n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col}')\n",
    "            plt.plot(\n",
    "                self.input_indices,\n",
    "                inputs[n, :, plot_col_index],\n",
    "                label='Inputs', marker='.', zorder=-10\n",
    "            )\n",
    "\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(\n",
    "                    self.label_indices, predictions[n, :, 0],\n",
    "                    marker='X', edgecolors='k', label='Predictions',\n",
    "                    c='#ff7f0e', s=64\n",
    "                )\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('Months')\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=24,\n",
    "        )\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39924656",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_WIDTH = 24\n",
    "LABEL_WIDTH = 1\n",
    "SHIFT = 1\n",
    "\n",
    "eval_window = WindowGenerator(\n",
    "    input_width=INPUT_WIDTH, label_width=LABEL_WIDTH,\n",
    "    shift=SHIFT, label_columns=[\"CPIAUCSL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4feb3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "eval_window.plot(max_subplots=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e3fe2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 250\n",
    "\n",
    "def compile_and_fit(model, window, patience=5):\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.losses.MeanSquaredError(),\n",
    "        optimizer=tf.optimizers.RMSprop(),\n",
    "    )\n",
    "\n",
    "\n",
    "    history = model.fit(\n",
    "        window.train, epochs=MAX_EPOCHS,\n",
    "        validation_data=window.val,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ae16f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**A simple prediction method**\n",
    "\n",
    "Domain expertise often can be used to create a \"baseline\" prediction model. For example,\n",
    "\n",
    "* The high temperature today could be somewhat accurately predicted by yesterday's high temperature.\n",
    "* The price of a stock today could be somewhat accurately predicted by the price of the stock yesterday.\n",
    "* U.S. annual inflation can somewhat accurately be predicted with a constant guess of 2%\n",
    "\n",
    "It's important to understand what baseline models are on the table because a simple baseline model will sometimes outperform a complex machine learning model (and when it does, we shouldn't be afraid to turn to the simple model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13fc8d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_Our baseline model_\n",
    "\n",
    "Predict a mixture of 0 (the mean that we normalized to) and the previous value:\n",
    "\n",
    "$$\\pi_t = (1 - \\gamma) 0.0 + \\gamma \\pi_{t-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275e752",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BaselineInflation(tf.keras.Model):\n",
    "    def __init__(self, gamma=0.9, label_index=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.label_index = label_index\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.label_index is None:\n",
    "            x = self.gamma*inputs + (1-self.gamma)*0\n",
    "        else:\n",
    "            x = self.gamma*inputs[:, :, self.label_index] + (1-self.gamma)*0\n",
    "\n",
    "        return x[:, :, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a283d1c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "baseline = BaselineInflation(gamma=0.9, label_index=eval_window.column_indices[\"CPIAUCSL\"])\n",
    "baseline.compile(loss=tf.losses.MeanSquaredError())\n",
    "\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "\n",
    "val_performance['Baseline'] = baseline.evaluate(eval_window.val)\n",
    "performance['Baseline'] = baseline.evaluate(eval_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d377a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wide_window_baseline = WindowGenerator(\n",
    "    input_width=INPUT_WIDTH, label_width=INPUT_WIDTH,\n",
    "    shift=SHIFT, label_columns=[\"CPIAUCSL\"]\n",
    ")\n",
    "\n",
    "wide_window_baseline.plot(baseline, max_subplots=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df67ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: 1D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3aa13f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Defining a 1D CNN\n",
    "\n",
    "Given hyper parameters,\n",
    "\n",
    "* $K$: Number of filters\n",
    "* $F$: Window width\n",
    "* $S$: Stride\n",
    "* $P$: Padding\n",
    "\n",
    "Let $w \\in \\mathbb{R}^K$ be the filter weights and $b \\in \\mathbb{R}$ be the bias.\n",
    "\n",
    "Output $i$ for a particular filter is defined by:\n",
    "\n",
    "$$z_i = \\left( \\sum_{j=0}^F w_{j} x_{i + j - F//2} \\right) + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d058b19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def conv1d(x, w, b, S=1, P=0):\n",
    "    \"\"\"\n",
    "    Apply single filter of 1d Convolution to x given\n",
    "    filter weights (w), bias (b), stride (S), and padding (P)\n",
    "    \"\"\"\n",
    "    assert P >= 0\n",
    "    if P == 0:\n",
    "        x_pad = x\n",
    "    else:\n",
    "        x_pad = np.concatenate([np.zeros(P), x, np.zeros(P)])\n",
    "    N = len(x)\n",
    "    F = len(w)\n",
    "    half_F = F // 2\n",
    "    out = []\n",
    "    for i in range(half_F, len(x_pad) - half_F, S):\n",
    "        window = x_pad[(i-half_F):(i+half_F + 1)]    \n",
    "        out.append((w @ window) + b)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f425272e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Predicting our data with a 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a4d4b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "CONV_WIDTH = 12\n",
    "\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=1,\n",
    "    shift=1\n",
    ")\n",
    "\n",
    "conv_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv1D(filters=8, kernel_size=(CONV_WIDTH,), activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(units=4, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(units=1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "compile_and_fit(conv_model, conv_window)\n",
    "\n",
    "val_performance['Conv1D'] = conv_model.evaluate(eval_window.val)\n",
    "performance['Conv1D'] = conv_model.evaluate(eval_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e2798",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "wide_window_conv = WindowGenerator(\n",
    "    input_width=36 + (CONV_WIDTH - 1),\n",
    "    label_width=36,\n",
    "    shift=SHIFT\n",
    ")\n",
    "\n",
    "wide_window_conv.plot(conv_model, max_subplots=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146cb7c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Similar to 1D CNNs, recurrent neural networks (RNNs) deal with sequentially ordered data.\n",
    "\n",
    "However, unlike CNNS, they can maintain a \"memory\" from beyond the current window/observation. Many people liken this internal memory to \"identifying a hidden state\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62e705",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simple RNN\n",
    "\n",
    "We begin by considering the most basic version of an RNN which we refer to as the \"simple RNN\"\n",
    "\n",
    "The output for step $t$ is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "  h_t &= \\sigma_h(W_h x_t + U_h h_{t-1} + b_h) \\\\\n",
    "  y_t &= \\sigma_y(W_y h_t + b_y) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc90521",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Simple RNN \"by hand\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977adc1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "timesteps = 100\n",
    "input_features = 32\n",
    "output_features = 64\n",
    "\n",
    "inputs = np.random.random((timesteps, input_features))\n",
    "state_t = np.zeros((output_features,))\n",
    "\n",
    "W = np.random.random((output_features, input_features))\n",
    "W_y = np.random.random((1, output_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features))\n",
    "b_y = 0.0\n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
    "    y_t = np.tanh(np.dot(W_y, output_t) + b_y)\n",
    "    successive_outputs.append(output_t)\n",
    "    state_t = output_t\n",
    "\n",
    "final_output_sequence = np.concatenate(successive_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54128fee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Using tensorflow**\n",
    "\n",
    "Let's turn to our predicting inflation problem and write a simple RNN using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a6aa2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "srnn_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.SimpleRNN(8, activation=\"relu\", return_sequences=False),\n",
    "        tf.keras.layers.Dense(4),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "compile_and_fit(srnn_model, eval_window)\n",
    "\n",
    "val_performance['SimpleRNN'] = srnn_model.evaluate(eval_window.val)\n",
    "performance['SimpleRNN'] = srnn_model.evaluate(eval_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b995caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7dad1e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSTM and GRU\n",
    "\n",
    "Similar to some of the problems we ran into when we had too many layers in our neural networks, simple RNNs often run into the [vanishing gradient problem and exploding gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).\n",
    "\n",
    "There have been two solutions proposed to address this concern:\n",
    "\n",
    "* Long short-term memory (LSTM), Hochreiter and Schmidhuber 1997\n",
    "* Gated recurrent unit (GRU), Cho et al. 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd82722",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**LSTM**\n",
    "\n",
    "LSTM introduces the notion of a \"carry state\".\n",
    "\n",
    "Output for step $t$ is now given by:\n",
    "\n",
    "$$h_t = \\sigma_h(W_h x_t + U_h h_{t-1} + V_h c_t + b_h)$$\n",
    "\n",
    "where $c_t$ is computed with\n",
    "\n",
    "\\begin{align*}\n",
    "  c_{t+1} &= i_t k_t + c_t f_t \\\\\n",
    "  i_t &= \\sigma(U_i \\hat{x}_{t} + W_i x_t + b_i) \\\\\n",
    "  f_t &= \\sigma(U_f \\hat{x}_{t} + W_f x_t + b_f) \\\\\n",
    "  k_t &= \\sigma(U_k \\hat{x}_{t} + W_k x_t + b_k) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Conceptually, this doesn't add a significant amount of complexity, but it does require more work from your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926de490",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lstm_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.LSTM(8, activation=\"relu\", return_sequences=False),\n",
    "        tf.keras.layers.Dense(4),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "compile_and_fit(lstm_model, eval_window)\n",
    "\n",
    "val_performance['LSTM'] = lstm_model.evaluate(eval_window.val)\n",
    "performance['LSTM'] = lstm_model.evaluate(eval_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9697b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae15d39",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**GRU**\n",
    "\n",
    "GRU is similar to the LSTM framework but is computationally slightly simpler. The pro is that this means it typically will run faster than LSTM but the drawback is that it can perform slightly worse.\n",
    "\n",
    "Output for step $t$ is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "  z_t &= \\sigma_g(W_z x_t + U_z h_{t-1} + b_z) \\\\\n",
    "  r_t &= \\sigma_g(W_r x_t + U_r h_{t-1} + b_r) \\\\\n",
    "  \\hat{h}_t &= \\phi_h(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h) \\\\\n",
    "  h_t &= (1 - z_t) \\odot h_{t-1} + z_t \\odot \\hat{h}_{t}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d067b6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gru_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.GRU(8, activation=\"relu\", return_sequences=False),\n",
    "        tf.keras.layers.Dense(4),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "compile_and_fit(gru_model, eval_window)\n",
    "\n",
    "val_performance['GRU'] = gru_model.evaluate(eval_window.val)\n",
    "performance['GRU'] = gru_model.evaluate(eval_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bcdafc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "val_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05acb72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stacking RNNs\n",
    "\n",
    "The convenient interface of Keras allows us to stack a variety of layers with one another. This includes using multiple levels of RNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3cbd3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "stacked_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.GRU(8, activation=\"relu\", return_sequences=True),\n",
    "        tf.keras.layers.GRU(4, activation=\"tanh\", return_sequences=False),\n",
    "        tf.keras.layers.Dense(4),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "compile_and_fit(stacked_model, eval_window, patience=25)\n",
    "\n",
    "val_performance['stacked'] = stacked_model.evaluate(eval_window.val)\n",
    "performance['stacked'] = stacked_model.evaluate(eval_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c341dcc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining 1D CNNs with RNNs\n",
    "\n",
    "A recommendation that is made in a few places is that CNNs and RNNs can be combined effectively. In \"Deep Learning with Python\", Francis Chollet makes the following observation,\n",
    "\n",
    "> Because 1D convnets process input patches independently, they aren't sensitive to the order of the timesteps (beyond a local scale...), unlike RNNs...\n",
    ">\n",
    ">One strategy to combine the speed and lightness of convnets with the order-sensitivity of RNNs is to use a 1D convnet as a preprocessing step before an RNN. This is especially beneficial when you're dealing with sequences that are so long that they can't realistically be processed with RNNs, such as sequences with thousands of steps. The convnet will turn the long input sequence into mcuh shorter sequences of higher-level features...\n",
    ">\n",
    ">This technique isn't seen often in research papers and practical applications possibly because it isn't well known. It's effective and ought to be more common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621240fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_WIDTH = 6\n",
    "\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=1,\n",
    "    shift=1\n",
    ")\n",
    "\n",
    "conv_rnn_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv1D(filters=4, kernel_size=(CONV_WIDTH,), activation=\"tanh\"),\n",
    "        tf.keras.layers.GRU(units=4, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(units=1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "compile_and_fit(conv_rnn_model, conv_window, patience=25)\n",
    "\n",
    "val_performance['Conv+RNN'] = conv_rnn_model.evaluate(eval_window.val)\n",
    "performance['Conv+RNN'] = conv_rnn_model.evaluate(eval_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5aff3a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc27360",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional resources\n",
    "\n",
    "* https://towardsdatascience.com/recurrent-neural-networks-for-recession-forecast-f435a2a4f3ef\n",
    "* https://medium.com/microsoftazure/neural-networks-for-forecasting-financial-and-economic-time-series-6aca370ff412\n",
    "* https://www.tensorflow.org/guide/keras/rnn\n",
    "* https://www.tensorflow.org/tutorials/structured_data/time_series#recurrent_neural_network"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
