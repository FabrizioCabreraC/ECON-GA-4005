{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a108e3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a5e2b0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "np.set_printoptions(linewidth=140, precision=4, suppress=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc5d72ca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fabrizio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fabrizio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc435162",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Text as data\n",
    "\n",
    "Language is constantly being stored online through academic papers, books, news articles, speeches, social media, product reviews, etc... This \"text data\", whether spoken or written, is one of the main sources of data accessible online.\n",
    "\n",
    "The ability to use and analyze this data would drastically increase the \"data available\" to data scientists, economists, and social scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb0d84",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What makes text difficult\n",
    "\n",
    "However, textual data comes with one main difficulty: The methods that we've discussed in this class (and in your other classes) are meant to deal with numerical data.\n",
    "\n",
    "In order to use text data (at scale) to gain insights into the world, we will need to find a way to transform the text data into numeric data. This transformation is difficult because language is a high dimensional object -- Words can have different meanings depending on how they are used and different combinations of words might map to the same meaning...\n",
    "\n",
    "This transformation and the subsequent analysis \"natural language processing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19769d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What we will learn today\n",
    "\n",
    "* Tokenization\n",
    "* Binary presence\n",
    "* Bag of words\n",
    "\n",
    "Additionally, we will introduce some new Python libraries including,\n",
    "\n",
    "* `nltk` (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b1227",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We are going to learn these using two \"fun\" examples as motivation. In both examples, we will attempt to associate text with its authors:\n",
    "\n",
    "1. [The Federalist Papers](https://en.wikipedia.org/wiki/The_Federalist_Papers) were a set of 85 articles written by Alexander Hamilton, James Madison, and John Jay under the pseudonym Publius. We have convincing evidence for who wrote a particular article for ~70 of the articles but the authorship of the other 15 is disputed. An early paper on NLP used classification methods to determine the author of the disputed articles. We view this as a \"canonical\" application of NLP.\n",
    "2. Our other example will be in a similar vein. Dr. Sargent has written extensively about various topics with various coauthors. We will use the abstracts from some of these papers to determine whether we can determine who he coauthored a paper with given just the abstract. Classifying abstracts to various co-authors will be the simpler of the two tasks and we will use it as a chance to help us build the tools needed to tackle the Federalist example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54cc3f7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/NLP/federalist.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-da2f6f3d5fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We downloaded this DataFrame from the `corpus` R package. Big thanks to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# authors of this package for making this easy!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfederalist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/NLP/federalist.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfederalist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/NLP/federalist.csv'"
     ]
    }
   ],
   "source": [
    "# We downloaded this DataFrame from the `corpus` R package. Big thanks to the\n",
    "# authors of this package for making this easy!\n",
    "federalist = pd.read_csv(\"Data/NLP/federalist.csv\", index_col=0)\n",
    "\n",
    "federalist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5daea",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Read in abstracts\n",
    "def read_abstract_file(fn):\n",
    "    author_abstract = []\n",
    "\n",
    "    with open(fn) as f:\n",
    "        foo = f.readlines()\n",
    "\n",
    "        for line in foo:\n",
    "            author, abstract = line.replace(\"\\n\", \"\").split(\",\", maxsplit=1)\n",
    "            author_abstract.append((author, abstract))\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        author_abstract,\n",
    "        columns=[\"coauthor\", \"abstract\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "labeled_abstracts = read_abstract_file(\"../../Data/NLP/labeled_abstracts.csv\")\n",
    "unlabeled_abstracts = read_abstract_file(\"../../Data/NLP/unlabeled_abstracts.csv\")\n",
    "\n",
    "labeled_abstracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e5a0c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Skimming the abstracts\n",
    "\n",
    "Can we by hand pick some themes in the research Dr. Sargent has done with his coauthors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194d46a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "foo = labeled_abstracts.query(\"coauthor == 'ljungqvist'\")[\"abstract\"]\n",
    "\n",
    "for (i, row) in foo.iteritems():\n",
    "    print(row)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c702c11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "The process by which we \"parse\" text and convert it into a smaller set of \"tokens\". Today, we'll convert the text by using words as tokens and will occasionally refer to this set of tokens as our \"vocabulary\".\n",
    "\n",
    "For example, consider the sentence,\n",
    "\n",
    ">The quick brown fox jumps over the lazy dog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552d8ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Unigram**\n",
    "\n",
    "One way we might \"tokenize\" this sentence would be to make each word its own token. This would give us the following tokens,\n",
    "\n",
    "1. The\n",
    "2. quick\n",
    "3. brown\n",
    "4. fox\n",
    "5. jumps\n",
    "6. over\n",
    "7. the\n",
    "8. lazy\n",
    "9. dog\n",
    "\n",
    "Our vocabulary would be 8 words with \"the\" appearing twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca32e0a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Bigram**\n",
    "\n",
    "Another way we might tokenize this sentence is to make each pair of words its own token. This would give us the following tokens\n",
    "\n",
    "1. The quick\n",
    "2. quick brown\n",
    "3. brown fox\n",
    "4. fox jumps\n",
    "5. jumps over\n",
    "6. over the\n",
    "7. the lazy\n",
    "8. lazy dog\n",
    "\n",
    "Generically we refer to this type of tokenization as an _N-gram_. It is useful because it allows words like \"very\" to be more context dependent (\"very good\" vs \"very bad\") but comes at the cost of increasing the dimensionality of the words considered.\n",
    "\n",
    "In practice unigram will typically work well for analysis as it keeps the vocabulary dimension small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94074c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Consider the following paper abstracts,\n",
    "\n",
    ">Ljungqvist and Sargent (1998, 2008) show that worse skill transition probabilities for workers who suffer involuntary layoffs (i.e., increases in turbulence) generate higher unemployment in a welfare state. den Haan, Haefke and Ramey (2005) challenge this finding by showing that if higher turbulence means that voluntary quits are also exposed to even a tiny risk of skill loss, then higher turbulence leads to lower unemployment within their matching model. We show (1) that there is no such brittleness of the positive turbulence-unemployment relationship in the matching model of Ljungqvist and Sargent (2007) even if we add such \"quit turbulence\", and (2) that if den Haan et al. had calibrated their productivity distribution to fit observed unemployment patterns that they miss, then they too would have found a positive turbulence-unemployment relationship in their model. Thus, we trace den Haan et al.'s finding to their assuming a narrower productivity distribution than Ljungqvist and Sargent had. Because den Haan et al. assume a distribution with such narrow support that it implies small returns to reallocating labor, even a small mobility cost shuts down voluntary separations. But that means that the imposition of a small layoff cost in tranquil times has counterfactually large unemployment suppression effects. When the parameterization is adjusted to fit historical observations on unemployment and layoff costs, a positive relationship between turbulence and unemployment reemerges.\n",
    "\n",
    ">To understand trans-Atlantic employment experiences since World War II, we build an overlapping generations model with two types of workers (high school and college graduates) whose different skill acquisition technologies affect their career decisions. Search frictions affect short-run employment outcomes. The model focuses on labor supply responses near beginnings and ends of lives and on whether unemployment and early retirements are financed by personal savings or public benefit programs. Higher minimum wages in Europe explain why youth unemployment has risen more there than in the U.S. Turbulence, in the form of higher risks of human capital depreciation after involuntary job destructions, causes long-term unemployment in Europe, mostly among older workers, but leaves U.S. unemployment unaffected. The losses of skill interact with workers' subsequent decisions to invest in human capital in ways that generate the age-dependent increases in autocovariances of income shocks observed by Moffitt and Gottschalk (1995).\n",
    "\n",
    "These are both drawn from papers coauthored by Lars Ljungqvist and Thomas Sargent. They are on similar topics so they share certain common words which should allow us to find some way to associate the two abstracts.\n",
    "\n",
    "* What difficulties might there be in performing this association?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a10af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Capitalization**\n",
    "\n",
    "We typically normalize all text to be lower-case so that we don't generate unintended mismatches. For example, we would want to acknowledge that `turbulence` appears in both abstracts but is capitalized in the second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0811b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Contractions**\n",
    "\n",
    "We will typically expand contractions into their full text meanings, i.e. \"don't\" -> \"do not\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b542870",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Numbers**\n",
    "\n",
    "Numbers without context in text don't always provide much useful information. For example, `(1)` and `(2)` would not help us understand what the abstract was about (and hence would not help us identify the author). On the other hand, there are contexts where the numbers are associated with years which might be helpful to us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3771b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Punctuation**\n",
    "\n",
    "In a \"fully-intelligent\" context, punctuation might help the computer break ideas into sentences, but, for what we do today, we will remove punctuation.\n",
    "\n",
    "One caveat to this is that there are many online contexts where subsets of punctuation marks can provide useful insights to what is being discussed -- For example, a tweet that contains `:-)` probably is a happy tweet whereas a tweet that contains `:-(` is likely a sad tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b9411",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Plural, prefixes/suffixes, verb conjugation/tense...**\n",
    "\n",
    "Certain words that we might want to be associated with one another seem to appear in multiple ways. For example,\n",
    "\n",
    "* `risk`, `risks`\n",
    "* `employment`, `unemployment`\n",
    "* `had`, `have`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ab3d7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Stop words**\n",
    "\n",
    "For many applications, common words such as `the`, `and`, and other similar words to not provide much information about what is being discussed. We often will remove these words to cut down the number of words in our \"vocabulary\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0592e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalization\n",
    "\n",
    "We discuss a few ways to address some of the issues we raised above.\n",
    "\n",
    "We want to emphasize that different normalization methods are useful for different contexts. Blindly applying all of these normalization methods is likely to lead to a suboptimal result as you begin to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee527d01",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_abstract = labeled_abstracts.loc[11, \"abstract\"]\n",
    "print(test_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c122e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Make text lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e6b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_abstract = test_abstract.lower()\n",
    "print(test_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f5243",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Remove numbers and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27804d2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_abstract = test_abstract.replace(\"-\", \" \")\n",
    "\n",
    "# I hadn't used `translate` before! Cool!\n",
    "test_abstract = test_abstract.translate(\n",
    "    str.maketrans(\"\", \"\", string.digits + string.punctuation)\n",
    ")\n",
    "\n",
    "print(test_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03be112",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Lemmatization vs Stemming\n",
    "\n",
    "In order to compare versions of similar words, we can use one of two methods:\n",
    "\n",
    "* Lemmatization: Processes the sentence using vocabulary and converts words into their base form (the \"lemma\").\n",
    "* Stemming: A crude set of heuristics that chops off the ends of words in the hope of achieving the goal some of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262f069",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pstem = PorterStemmer()\n",
    "\n",
    "print(\" \".join(list(map(pstem.stem, test_abstract.split(\" \")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a25c3d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "test_abstract = \" \".join(\n",
    "    map(lemm.lemmatize, test_abstract.split(\" \"))\n",
    ")\n",
    "print(test_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d54b08",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da20c9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "\n",
    "    # Get list of stopwords\n",
    "    sw = stopwords.words(\"english\")\n",
    "    # Allocate space for all of our words\n",
    "    out = []\n",
    "\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in sw:\n",
    "            out.append(word)\n",
    "\n",
    "    return \" \".join(out)\n",
    "\n",
    "print(remove_stopwords(test_abstract))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f10df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalizing all abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc64ee8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def normalizer(text):\n",
    "    # First, set all text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Now remove punctuation and numbers\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.translate(\n",
    "        str.maketrans(\"\", \"\", string.digits + string.punctuation)\n",
    "    )\n",
    "\n",
    "    # Remove anywhere with more than two consecutive whitespaces\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemm = WordNetLemmatizer()\n",
    "    text = \" \".join(\n",
    "        map(lemm.lemmatize, text.split(\" \"))\n",
    "    )\n",
    "\n",
    "    # Remove stopwords\n",
    "    sw = stopwords.words(\"english\")\n",
    "    out = []\n",
    "\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in sw:\n",
    "            out.append(word)\n",
    "\n",
    "    return \" \".join(out)\n",
    "\n",
    "labeled_abstracts[\"clean_abstract\"] = labeled_abstracts[\"abstract\"].map(normalizer)\n",
    "unlabeled_abstracts[\"clean_abstract\"] = unlabeled_abstracts[\"abstract\"].map(normalizer)\n",
    "labeled_abstracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0391be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary presence\n",
    "\n",
    "Once we've tokenized our data, we must choose a way to convert our tokens into numeric data. The simplest way to do this is simply categorize whether or not a value was present in the text or not.\n",
    "\n",
    "Imagine that our vocabulary is `[\"quick\", \"brown\", \"fox\", \"red\", \"dog\", \"Clifford\"]` then the sentence \"The quick brown fox jumps over the lazy brown dog\" would get converted to numeric data by placing a 1 for each vocabulary word that matches any token in the tokenized data. This would result in `[1, 1, 1, 0, 1, 0]`\n",
    "\n",
    "Even though the word `\"brown\"` showed up multiple times, it just gets labeled as a 1 because we are just testing for the presence of certain words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe98e1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def binary_presence(vocabulary, text):\n",
    "    # Allocate memory\n",
    "    out = np.zeros(len(vocabulary), dtype=int)\n",
    "\n",
    "    # Check whether the word is in our text\n",
    "    for (iv, v) in enumerate(vocabulary):\n",
    "        out[iv] = 1 if v in text else 0\n",
    "\n",
    "    return out\n",
    "\n",
    "# Build our test vocabulary\n",
    "vocabulary = [\"quick\", \"brown\", \"fox\", \"red\", \"dog\", \"Clifford\"]\n",
    "text = \"the quick brown fox jumps over the lazy brown dog\"\n",
    "\n",
    "# Check whether this lines up with expected result\n",
    "binary_presence(vocabulary, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e276ba4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing vocabulary\n",
    "\n",
    "Binary presence seems easy enough to implement! However, we are left with one remaining detail... How do we choose a vocabulary?\n",
    "\n",
    "One choice is to take _ALL_ words possible, but that will leave us with many words that might add a limited amount of information.\n",
    "\n",
    "The way that we will do it in this example is to find all of the words that show up in at least 3 abstracts. We have at least 4 abstracts from a paper written with each coauthor so words that are used frequently in abstracts with particular coauthors should show if we count words that are in 3 abstracts.\n",
    "\n",
    "Ultimately, the way that you build your vocabulary will be up to you (and be usage dependent!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae83e1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Start with all words\n",
    "all_words = list(\n",
    "    set(\n",
    "        labeled_abstracts[\"clean_abstract\"].map(\n",
    "            lambda x: x.split(\" \")\n",
    "        ).sum()\n",
    "    )\n",
    ")\n",
    "all_words.sort()\n",
    "\n",
    "# Count the number of times each word shows up\n",
    "# in an abstract\n",
    "out = np.zeros(len(all_words), dtype=bool)\n",
    "for (iw, word) in enumerate(all_words):\n",
    "    count_abstracts = (\n",
    "        labeled_abstracts[\"clean_abstract\"]\n",
    "        .str\n",
    "        .contains(word)\n",
    "        .sum()\n",
    "    )\n",
    "    out[iw] = count_abstracts >= 3\n",
    "    \n",
    "abstract_vocabulary = [\n",
    "    word for (iw, word) in enumerate(all_words) if out[iw]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9a12d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Naive Bayesian classification\n",
    "\n",
    "Our favorite law (Bayes law) shows up again! We want to classify whether an abstract was written with a particular coauthor given which words from our vocabulary show up in the abstract.\n",
    "\n",
    "Let $y_j$ denote each coauthor (Tim Cogley, Lars Hansen, or Lars Ljungqvist) and $x_i$ denote whether or not each word from the vocabulary appeared in the abstract, then:\n",
    "\n",
    "$$P(y_j | x_1, x_2, \\dots, x_n) = \\frac{P(y) P(x_1, x_2, \\dots, x_n | y_j)}{P(x_1, x_2, \\dots, x_n)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf372a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Why \"naive\"?**\n",
    "\n",
    "We refer to this classification method as \"naive\" because we assume that, given the coauthor that the probability of observing each word is independent to the others.\n",
    "\n",
    "$$P(x_1, x_2, \\dots, x_n | y_j) = P(x_1 | y_j) P(x_2 | y_j) \\dots P(x_n | y_j)$$\n",
    "\n",
    "There is no basis for this assumption and is simply made to simplify the mathematics. In fact, we could probably argue that this is a bad assumption for our data, but, in spite of this, it will prove to be successful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d3c29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Parameter estimation**\n",
    "\n",
    "We will assume that an abstract is equally likely to have been written by each coauthor, i.e., $P(y_j) = P(y_k) = \\frac{1}{3}$\n",
    "\n",
    "The probabilities $P(x_i | y_j)$ are computed by using something similar to a beta-binomial conjugate prior (but allows for more than 2 classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4dce09",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4d1e6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nobs = labeled_abstracts.shape[0]\n",
    "nwords = len(abstract_vocabulary)\n",
    "\n",
    "coauthors = {\n",
    "    \"cogley\": 0,\n",
    "    \"hansen\": 1,\n",
    "    \"ljungqvist\": 2\n",
    "}\n",
    "\n",
    "X = np.zeros((nobs, nwords), dtype=int)\n",
    "y = np.zeros(nobs, dtype=int)\n",
    "\n",
    "for i in range(nobs):\n",
    "    y[i] = coauthors[labeled_abstracts.at[i, \"coauthor\"]]\n",
    "\n",
    "    for (j, word) in enumerate(abstract_vocabulary):\n",
    "        X[i, j] = word in labeled_abstracts.at[i, \"clean_abstract\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db375c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Estimate using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9704f6b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(fit_prior=False)\n",
    "\n",
    "mnb.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e92760a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Can we predict who the remaining abstracts were written with?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf82754",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for (i, ab) in unlabeled_abstracts[\"abstract\"].iteritems():\n",
    "    print(ab)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc89fc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "noos = unlabeled_abstracts.shape[0]\n",
    "\n",
    "X_oos = np.zeros((noos, nwords), dtype=int)\n",
    "y_oos = np.zeros(noos, dtype=int)\n",
    "\n",
    "for i in range(noos):\n",
    "    y_oos[i] = coauthors[unlabeled_abstracts.at[i, \"coauthor\"]]\n",
    "\n",
    "    for (j, word) in enumerate(abstract_vocabulary):\n",
    "        X_oos[i, j] = word in unlabeled_abstracts.at[i, \"clean_abstract\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7163da",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mnb.predict(X_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aaba53",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e6146",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mnb.predict_proba(X_oos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796687a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of words\n",
    "\n",
    "[Bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) will be similar to binary presence, but, rather than simple track whether a word shows up in the text, we will count the number of times it shows up.\n",
    "\n",
    "The \"implicit assumption\" in bag of words models is that words are drawn from an urn with certain probabilities and that order is unimportant.\n",
    "\n",
    "We revisit our example from before:\n",
    "\n",
    "Imagine that our vocabulary is `[\"quick\", \"brown\", \"fox\", \"red\", \"dog\", \"Clifford\"]` then the sentence \"The quick brown fox jumps over the lazy brown dog\" would get converted to numeric data by counting the number of times each vocabulary word appears in the tokenized data. This would result in `[1, 2, 1, 0, 1, 0]`\n",
    "\n",
    "Before, we placed a 1 in the vector element that corresponded to brown, but since we are now counting occurrences, we place a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e243a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def bag_of_words(vocabulary, text):\n",
    "    # Check whether the word is in our text\n",
    "    dcount = {word: 0 for word in vocabulary}\n",
    "    for word in text.split(\" \"):\n",
    "        if word in vocabulary:\n",
    "            dcount[word] += 1\n",
    "\n",
    "    out = np.array([dcount[word] for word in vocabulary])\n",
    "\n",
    "    return out\n",
    "\n",
    "# Build our test vocabulary\n",
    "vocabulary = [\"quick\", \"brown\", \"fox\", \"red\", \"dog\", \"Clifford\"]\n",
    "text = \"the quick brown fox jumps over the lazy brown dog\"\n",
    "\n",
    "# Check whether this lines up with expected result\n",
    "bag_of_words(vocabulary, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f692e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing vocabulary\n",
    "\n",
    "We must again consider how to choose our vocabulary. The Federalist papers were all written on a similar topic, so many of the words will be shared across the documents (one of the reasons why binary presence will not perform particularly well in this case).\n",
    "\n",
    "Rather than target the \"topic\" covered in each document, we will attempt to extract information on the writing style by tracking the way certain words are used. In the original paper, Mosteller and Wallace choose 30 (relatively common) words that they think can proxy for each of the individual authors' writing styles. They call these words \"markers\" and they highlight that Hamilton and Madison use words differently. For example, In the 14 essays written by Madison, the word _while_ never occurs but _whilst_ occurs in 8 of them. Similarly, _while_ occurs in 15 of the 48 Hamilton essays, but _whist_ never occurs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9942fa4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "federalist_vocabulary = [\n",
    "    \"upon\", \"also\", \"an\", \"by\", \"of\", \"on\", \"there\", \"this\", \"to\",\n",
    "    \"although\", \"both\", \"enough\", \"while\", \"whilst\", \"always\", \"though\",\n",
    "    \"commonly\", \"consequently\", \"considerable\", \"according\", \"apt\",\n",
    "    \"direction\", \"innovation\", \"language\", \"vigor\", \"kind\", \"matter\",\n",
    "    \"particularly\", \"probability\", \"work\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548fb00f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalizing the data\n",
    "\n",
    "Given that our goals are slightly different this time, our normalization procedure will look slightly different.\n",
    "\n",
    "In particular, we will _not_ want to remove stopwords because many of the vocabularly words that we're using to identify the styles are stopwords.\n",
    "\n",
    "We will also directly replace some of the various tenses of words rather than use a lemmatizer or stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7a624",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def normalizer_federalist(text):\n",
    "    # First, set all text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Now remove punctuation and numbers\n",
    "    text = text.translate(\n",
    "        str.maketrans(\"\", \"\", string.digits + string.punctuation)\n",
    "    )\n",
    "\n",
    "    # Remove anywhere with more than two consecutive whitespaces\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    # Replace certain words\n",
    "    replacer = [\n",
    "        (\"matters\", \"matter\"),\n",
    "        (\"considerably\", \"considerable\"),\n",
    "        (\"innovations\", \"innovation\"),\n",
    "        (\"vigorous\", \"vigor\"),\n",
    "        (\"works\", \"work\")\n",
    "    ]\n",
    "    for (_w, _r) in replacer:\n",
    "        text.replace(_w, _r)\n",
    "\n",
    "    return text\n",
    "\n",
    "federalist[\"clean_text\"] = federalist[\"text\"].map(\n",
    "    normalizer_federalist\n",
    ")\n",
    "\n",
    "federalist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81f258",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Additionally, rather than keep the counts directly, we will convert the counts to a rate per 100,000 words and round to the nearest integer. We do this to make sure that the documents are comparable in spite of differing in length. We convert to integers because our naive Bayes classifier assumes that the inputs are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff10d7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "federalist[\"nwords\"] = federalist[\"clean_text\"].str.split(\" \").map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c804a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26956fb2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Separate into known and unknown authors\n",
    "known_federalist = (\n",
    "    federalist\n",
    "    .loc[~federalist[\"author\"].isna(), :]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "unknown_federalist = (\n",
    "    federalist\n",
    "    .loc[federalist[\"author\"].isna(), :]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "nobs = known_federalist.shape[0]\n",
    "nvocab = len(federalist_vocabulary)\n",
    "\n",
    "publius = {\n",
    "    \"Hamilton\": 0,\n",
    "    \"Jay\": 1,\n",
    "    \"Madison\": 2\n",
    "}\n",
    "\n",
    "X = np.zeros((nobs, nvocab), dtype=int)\n",
    "y = np.zeros(nobs, dtype=int)\n",
    "\n",
    "for i in range(nobs):\n",
    "    y[i] = publius[known_federalist.at[i, \"author\"]]\n",
    "    X[i, :] = bag_of_words(\n",
    "        federalist_vocabulary,\n",
    "        known_federalist.at[i, \"clean_text\"]\n",
    "    )\n",
    "\n",
    "# Convert to rates\n",
    "nwords = known_federalist.loc[:, \"nwords\"].to_numpy()\n",
    "X = np.round(100_000 * X/nwords[:, None]).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406dd9d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Estimate with sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8192e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(fit_prior=False)\n",
    "\n",
    "mnb.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3b811",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Predict unknown authors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecea840",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "noos = unknown_federalist.shape[0]\n",
    "\n",
    "X_oos = np.zeros((noos, nvocab), dtype=int)\n",
    "\n",
    "for i in range(noos):\n",
    "    X_oos[i, :] = bag_of_words(\n",
    "        federalist_vocabulary,\n",
    "        unknown_federalist.at[i, \"clean_text\"]\n",
    "    )\n",
    "\n",
    "# Convert to rates\n",
    "nwords = unknown_federalist.loc[:, \"nwords\"].to_numpy()\n",
    "X_oos = np.round(100_000 * X_oos/nwords[:, None]).astype(int)\n",
    "\n",
    "probs_oos = mnb.predict_proba(X_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_h = known_federalist.query(\"author == 'Hamilton'\").index\n",
    "wpht_h = X[idx_h, :].mean(axis=0)\n",
    "\n",
    "idx_m = known_federalist.query(\"author == 'Madison'\").index\n",
    "wpht_m = X[idx_m, :].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a49467",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2)\n",
    "\n",
    "ax[0].bar(np.arange(nvocab), wpht_h)\n",
    "ax[0].set_ylim(0, 500)\n",
    "\n",
    "ax[1].bar(np.arange(nvocab), wpht_m)\n",
    "ax[1].set_ylim(0, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe8547",
   "metadata": {},
   "outputs": [],
   "source": [
    "federalist_vocabulary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb7c4d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "probs_oos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff615e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* Business Data Science. Matt Taddy. (Chapter 8).\n",
    "* Inference and Disputed Authorship: The Federalist. Mosteller, Frederick and Wallace, David L. https://www.jstor.org/stable/202633\n",
    "* Updating: A Set of Bayesian Notes. Jeffrey B. Arnold. (Chapter 4). https://jrnold.github.io/bayesian_notes/index.html\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
