{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720d6dfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing (Part 2)\n",
    "\n",
    "Many of the examples and pieces of the code are taken from the \"Deep Learning with Python\" book by Francois Chollet... We've recommended it before, but it is a well done book so let us recommend it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5a169",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=140)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab66c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9b135",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text data can be challenging to work with\n",
    "\n",
    "Text data introduces a variety of unique challenges, some of which we've already discussed in previous lectures. We summarize some of these challenges below,\n",
    "\n",
    "* Textual data is inherently _not_ numeric while the methods we have learned require numeric data\n",
    "* The same word can be used differently in different contexts, i.e. \"The new Apple product is great.\" vs \"This apple is delicious\"\n",
    "* Text data is high dimensional -- Most languages have between 100,000 and 1,000,000 words.\n",
    "* Text data is often not structured in the same way that we're used to. How should we account for document structure like sentences, paragraphs, and quotes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039a504",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review:\n",
    "\n",
    "Let's quickly review some of the concepts from our previous lecture on NLP.\n",
    "\n",
    "Our main goal was to \"tokenize\" the text that we were working with. Tokenization is the process by which we convert the textual data into a numeric representation. In our last notebook, we did this in two steps:\n",
    "\n",
    "* Preprocessing\n",
    "* Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e5606",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "In order to process textual data, we needed a way to convert it into numerical data. The first step of this was to preprocess the data by separating the words and normalizing them via a few steps, i.e.\n",
    "\n",
    "> The quick brown fox jumped over the lazy dog.\n",
    "\n",
    "became\n",
    "\n",
    "```python\n",
    "processed_text = [\n",
    "    \"the\", \"quick\", \"brown\", \"fox\",\n",
    "    \"jump\", \"over\", \"the\", \"lazy\", \"dog\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4a5ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bag of words\n",
    "\n",
    "Once we had processed the text, we used two versions of the _bag of words_ algorithm. In a bag of words style algorithm for converting text to numbers, we ignore the order of the words and simply consider an $N$ element vector where $N$ is the \"size of our vocabulary\". The elements of this vector are assigned based on the algorithm used.\n",
    "\n",
    "* Binary presence: Each word becomes an element of the vector and receives a 1 if the word appears at all in the text and a 0 if the word does not appear in the text\n",
    "* N-gram: The elements of the vector correspond to either the number of times that a particular word appears or the frequency with which it appears"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ffd30",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Shortcomings**\n",
    "\n",
    "The main shortcoming of this style of algorithm is that it ignores order! Ignoring order means that it will be difficult to distinguish between how the word \"apple\" is used in different contexts like from the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ac888",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Keras makes both of these easy and we should use their functionality whenever possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156d771",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The quick red fox jumps over the sleepy dog\",\n",
    "    \"The quick brown fox finds the destructive groundhogs\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f7f08",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=14)\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184c5eb",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.texts_to_matrix(sentences, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a10e7",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.texts_to_matrix(sentences, mode=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeadd9d6",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.texts_to_matrix(sentences, mode=\"freq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a4061d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced tokenization\n",
    "\n",
    "We discuss two additional methods that can be used to tokenize our data:\n",
    "\n",
    "* one-hot encoding\n",
    "* word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed54b92b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### One-hot encoding\n",
    "\n",
    "One-hot encoding methods are the same idea as how one-hot encoding is used for categorical data. We consider $N$ unique possible words and then embed the sentence using vectors that have many 0s and a single 1.\n",
    "\n",
    "For example,\n",
    "\n",
    "> The quick brown fox jumped over the lazy dog\n",
    "\n",
    "would be expressed by:\n",
    "\n",
    "```python\n",
    "tokenized_sentence = np.array(\n",
    "    [[1, 0, 0, 0, 0, 0, 0, 0],  # The\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0],  # quick\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],  # brown\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0],  # fox\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0],  # jump\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0],  # over\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0],  # the\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0],  # lazy\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1]],  # dog\n",
    ")\n",
    "```\n",
    "\n",
    "Make note that the columns could be in a different order. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ae569",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Implementing one-hot encoding**\n",
    "\n",
    "While it would be relatively straightforward to write our own version of one-hot encoding, `keras` takes care of various details for us automatically and we should leverage these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c1d46",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"The quick brown fox jumped over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(sentence, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c066b",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "to_categorical(\n",
    "    one_hot(sentence, n=9)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343f804",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wait... This doesn't look right.\n",
    "\n",
    "Under the hood, keras is actually using one-hot hashing. If the number of words in your dictionary is \"too small\" then you can wind up with \"hash collisions\" which makes the algorithm think that two distinct words are the same... We found this on [StackOverflow](https://stackoverflow.com/questions/66507613/confused-by-output-of-keras-text-preprocessing-one-hot)\n",
    "\n",
    "Let's try with a larger vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1370b0d",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Choose n to be very large so that we don't have collisons\n",
    "word_nums = one_hot(sentence, n=1000)\n",
    "my_encoding = dict(\n",
    "    zip(word_nums, range(len(word_nums)))\n",
    ")\n",
    "\n",
    "ohe = to_categorical(\n",
    "    np.array([my_encoding[x] for x in word_nums])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d0154",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512ec97",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sparsity of one-hot encoding representations**\n",
    "\n",
    "The output of the one-hot encoding tokenization is a very large (but very sparse!) representation of the text. There can be no more than one non-zero element per row.\n",
    "\n",
    "This is a relatively inefficient way to store the data... Is there a way to lower the dimensionality?\n",
    "\n",
    "Yes! By using something called \"word embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c436512",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word embeddings\n",
    "\n",
    "Word embeddings are an alternative to one-hot encoding representations of text. Whereas one-hot encoding is exceptionally high dimensional and sparse, word embeddings aim to be lower dimensional but dense.\n",
    "\n",
    "One can obtain a word embedding in one of two ways:\n",
    "\n",
    "1. Use a prepackaged word embedding model trained by someone else (something akin to transfer learning)\n",
    "2. Learn a word embedding in conjunction with the main task that you're attempting to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7600a25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Desired properties of an embedding**\n",
    "\n",
    "1. Similar words produce similar outputs. \"jog\" and \"run\" and \"frog\" and \"toad\" should produce similar vectors\n",
    "2. Linear substructures. The canonical example is `\"king\" - \"man\" + \"woman\" = \"queen\"`\n",
    "3. Sufficiently low dimensional to be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac987bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Common word embedding models**\n",
    "\n",
    "There are a few word embedding models that are commonly used:\n",
    "\n",
    "* Google's `Word2Vec` model trained on their Google News dataset which had ~100 billion words\n",
    "* GloVe is a model trained by researchers at Stanford using a different methodology than `Word2Vec`\n",
    "\n",
    "We will use the Google News `word2vec` model to illustrate word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f6081",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Big file! Will take a bit to download/load\n",
    "w2v = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ff800",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "w2v.get_vector(\"queen\").shape  # 300 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e7f13",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def display_pca_scatterplot(model, words):\n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:, :2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b58cd6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "display_pca_scatterplot(\n",
    "    w2v,\n",
    "    [\n",
    "        \"man\", \"woman\",\n",
    "        \"king\", \"queen\",\n",
    "        \"prince\", \"princess\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df39cd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "display_pca_scatterplot(\n",
    "    w2v,\n",
    "    [\n",
    "        \"walk\", \"jog\", \"run\",\n",
    "        \"frog\", \"toad\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842055b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "display_pca_scatterplot(\n",
    "    w2v,\n",
    "    [\n",
    "        \"tall\", \"taller\", \"tallest\",\n",
    "        \"long\", \"longer\", \"longest\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ef2f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training our own embedding\n",
    "\n",
    "We can also learn our own embeddings based on a specific task.\n",
    "\n",
    "In this example, we'll use IMDB movie review text to classify reviews as either positive or negative. We will use preprocessed data that keras provides for us.\n",
    "\n",
    "* The `x` data contains an array or lists of integers. Each element of the array represents a single review and the list of integers is used to encode the words used in the review. For example, if `\"test\" -> 1` and `\"case\" -> 2` then `np.array([list(1, 2, 1), list(2, 1, 2)])` would represent two reviews with the words `\"test case test\"` and `\"case test case\"`.\n",
    "* The `y` data contains an array of 0s and 1s. If element $i$ of this array is 1 (0) then review $i$ was positive (negative).\n",
    "\n",
    "When we load the data, we specify the number of words that we would like to (which will keep the $n$ most commonly used words) and we then keep only the first $m$ words of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b41e66",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nvocab = 10_000\n",
    "nkeep = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    num_words=nvocab\n",
    ")\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=nkeep)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=nkeep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a6977",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`Embedding` layer**\n",
    "\n",
    "The `Embedding` layer takes an input of integers of shape `(samples, sequence_length)` and converts them into vectors of floats with size `(samples, sequence_length, embedding_dimensionality)`.\n",
    "\n",
    "It does this by mapping each unique integer into its own pre-specified floating point vector. For example, assume the word `good` might be represented by integer `1` and be mapped to `[0.1, 0.2, 0.3]` and word `bad` was represented by integer `2` and was mapped to `[-0.1, -0.2, -0.3]`. Then a sample of two reviews that said `\"good good good\"` and `\"bad bad bad\"` would then be represented by\n",
    "\n",
    "```python\n",
    "np.array([list(1, 1, 1), list(2, 2, 2)])\n",
    "```\n",
    "\n",
    "and would be assigned an embedding of\n",
    "\n",
    "```python\n",
    "np.array([\n",
    "    [[0.1, 0.2, 0.3], [0.1, 0.2, 0.3], [0.1, 0.2, 0.3]],\n",
    "    [[-0.1, -0.2, -0.3], [-0.1, -0.2, -0.3], [-0.1, -0.2, -0.3]\n",
    "])\n",
    "```\n",
    "\n",
    "Training the embedding layer attempts to find these vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a232d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's train an embedding for our IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29add72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nkeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ff6fd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = tf.keras.Sequential(\n",
    "    [\n",
    "        # Embedding layer\n",
    "        tf.keras.layers.Embedding(nvocab, 32, input_length=nkeep),\n",
    "        # Converts from 3D to 2d of shape (samples, maxlen*32)\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "embedding_model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a007d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07aa424",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "history = embedding_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10, batch_size=64,\n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87dd7e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_acc_loss_plot(history):\n",
    "\n",
    "    epoch = history.epoch\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    # Accuracy\n",
    "    ax[0].plot(epoch, history.history[\"acc\"], linestyle=\"-.\", label=\"Training\")\n",
    "    ax[0].plot(epoch, history.history[\"val_acc\"], linestyle=\"-\", label=\"Validation\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(\"Model accuracy\")\n",
    "\n",
    "    # Loss\n",
    "    ax[1].plot(epoch, history.history[\"loss\"], linestyle=\"-.\")\n",
    "    ax[1].plot(epoch, history.history[\"val_loss\"])\n",
    "    ax[1].set_title(\"Loss\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "make_acc_loss_plot(history);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bfa292",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we do better?\n",
    "\n",
    "The dense layer we used in training our own embedding seems to have performed relatively well, but the dense layer only observes each word as a separate entity and ignores the fact that combinations of words might mean something...\n",
    "\n",
    "Do we know any methods that allow us to analyze data sequentially?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c557a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Recurrent neural networks strike again!**\n",
    "\n",
    "The other main application of recurrent neural networks is text analysis because they use \"memory\" to understand the context of certain sentences.\n",
    "\n",
    "In our example, the sentence, \"This move is the bomb\" is much different than the sentence, \"This movie is a bomb\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b4335",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_Simple RNN_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556e2a0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "simple_rnn = tf.keras.Sequential(\n",
    "    [\n",
    "        # Embedding layer\n",
    "        tf.keras.layers.Embedding(nvocab, 32, input_length=nkeep),\n",
    "        tf.keras.layers.SimpleRNN(8, return_sequences=False),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "simple_rnn.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "simple_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495c29a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "simple_rnn_history = simple_rnn.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10, batch_size=64,\n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea10579",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "make_acc_loss_plot(simple_rnn_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee788660",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_LSTM RNN_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9cb6ba",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lstm_rnn = tf.keras.Sequential(\n",
    "    [\n",
    "        # Embedding layer\n",
    "        tf.keras.layers.Embedding(nvocab, 32, input_length=nkeep),\n",
    "        tf.keras.layers.LSTM(8, return_sequences=False),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "lstm_rnn.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "lstm_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9a665",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lstm_rnn_history = lstm_rnn.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10, batch_size=64,\n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2a7dc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "make_acc_loss_plot(lstm_rnn_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf45b5e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_GRU RNN_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291a11e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gru_rnn = tf.keras.Sequential(\n",
    "    [\n",
    "        # Embedding layer\n",
    "        tf.keras.layers.Embedding(nvocab, 32, input_length=nkeep),\n",
    "        tf.keras.layers.GRU(8, return_sequences=False),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "gru_rnn.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "gru_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b3b9e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gru_rnn_history = gru_rnn.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10, batch_size=64,\n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a84bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b977fd4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "make_acc_loss_plot(gru_rnn_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765217c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Challenge: Train a better model\n",
    "\n",
    "I'm going to restrict myself to about 10 minutes to train a better model for sentiment analysis using a RNN. I'll post the output of each of my models below and we can talk about why I tried some of the things that I tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b8bc8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def test_a_model(model):\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=10, batch_size=64,\n",
    "        validation_data=(x_test, y_test)\n",
    "    )\n",
    "\n",
    "    make_acc_loss_plot(history)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fff7d8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model_1 = tf.keras.Sequential(\n",
    "    [\n",
    "        # Embedding layer\n",
    "        tf.keras.layers.Embedding(nvocab, 10, input_length=nkeep),\n",
    "        tf.keras.layers.GRU(4, return_sequences=False),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_a_model(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a32890",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model_2 = tf.keras.Sequential(\n",
    "    [\n",
    "        # Embedding layer\n",
    "        tf.keras.layers.Embedding(nvocab, 10, input_length=nkeep),\n",
    "        tf.keras.layers.GRU(8, dropout=0.1, return_sequences=False),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_a_model(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db937ab",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model_3 = tf.keras.Sequential(\n",
    "    [\n",
    "        # Embedding layer\n",
    "        tf.keras.layers.Embedding(nvocab, 10, input_length=nkeep),\n",
    "        tf.keras.layers.GRU(8, return_sequences=True),\n",
    "        tf.keras.layers.GRU(2, dropout=0.1, return_sequences=False),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_a_model(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = tf.keras.Sequential(\n",
    "    [\n",
    "        # Embedding layer\n",
    "        tf.keras.layers.Embedding(nvocab, 20, input_length=nkeep),\n",
    "        tf.keras.layers.GRU(8, return_sequences=True),\n",
    "        tf.keras.layers.GRU(2, dropout=0.1, return_sequences=False),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_a_model(model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0deb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = tf.keras.Sequential(\n",
    "    [\n",
    "        # Embedding layer\n",
    "        tf.keras.layers.Embedding(nvocab, 10, input_length=nkeep),\n",
    "        tf.keras.layers.Dense(8, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_a_model(model_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ed242",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* [Gensim documentation](https://radimrehurek.com/gensim/index.html)\n",
    "* [GloVe documentation](https://nlp.stanford.edu/projects/glove/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
